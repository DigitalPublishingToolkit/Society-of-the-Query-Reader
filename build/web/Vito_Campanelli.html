<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="SotQreader.web.css" type="text/css" />
</head>
<body>
<p>In recent months, revelations about the existence of the surveillance program called PRISM and the consequent scandal (the ‘datagate’) that has troubled the administration of U.S. President Obama has caused an outbreak of public debate on the internet. Our awareness of programs that allow some U.S. and European intelligence agencies to spy on communications on the web (in addition to telephonic ones) has been increasing. However, it should be noted that very often the worst risks to the privacy of netizens do not come from the uncertain and contradictory policies of Western governments, but rather from some unexpected protagonists: the web companies that manage and organize the online spaces within which we spend an increasingly significant part of our days, transmitting huge amounts of data related to our personal spheres. The most famous and popular social network, Facebook, whose founder, Mark Zuckerberg, announced in the fall of 2011 the launch of a new sharing philosophy called 'frictionless sharing', provides an emblematic example of this.</p>
<p>The event was celebrated at Facebook F8, a conference organized by Facebook in a rather irregular fashion (no conferences were held in 2009, 2012, and 2013) to bring together developers and companies that make products and services that have been integrated into the social network. During these meetings, Facebook has frequently introduced new features. At the 2011 conference the most important novelty was the so-called ‘timeline’, a reorganization of the interface so that the history of users' activity is shown on their profile. Compared to such an authentic revolution in user experience, Zuckerberg's reference to**frictionless sharing slipped into the background.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> ^^Indeed, only the most attentive participants caught the important implications of the approach underlying this conception.</p>
<p>The premise of this new philosophy of sharing is that one will no longer need to do the work of sharing tastes and preferences. Facebook, in fact, will register tastes automatically, thus making them available to the contacts (friends) of each user. This spur to conformism would be limited for the time being to media objects (movies, music, books, etc.) published in the most popular walled garden<em>,</em> but clearly, in the near future, nothing could stop Facebook from also recording consumption choices and preferences relative to all the other places visited by a user on the internet (and available technologies already allow such monitoring).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Evgeny Morozov is among those who noticed this claim by Facebook's founder. Never tired of emphasizing the weaknesses of internet culture, he perfectly captured that the most interesting aspect of this feature is the ideological one: Facebook's propaganda efforts aim to present an extreme form of sharing as something normal and even desirable. However, it is precisely this <em>sharing totalitarianism</em> that 'is killing <em>cyberflânerie</em>', because 'the whole point of the <em>flâneur</em>'s wanderings' is that one does not know what he or she cares about.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>On closer inspection, the functioning of Facebook’s timeline is similar to Google Now (the ‘intelligent and personal’ information service that supposedly predicts what you want to know), and in fact i t uses the potentialities of the most popular search engine, along with localization and access to personal user data, to automatically offer information and news about the context in which one is situated. The idea in this case is that such tools, which automatically organize the information we need, ‘free’ us, as they allow us to focus 'on what's important' to us. <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Such a view implies that, according to Google, information selection should not be considered a core activity in the lives of human beings, but a burden that one may well leave to machines and their algorithms.</p>
<p>By crossing seemingly trivial data – for example, books or records purchased online – Facebook's tools can help determine, quite precisely, a profile of the observed user and then establish their political orientation, gender, religion, etc.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> Beyond legitimate concerns for this additional threat to privacy (already severely under attack in social networks), perhaps an even greater danger should be considered: frictionless sharing. In fact, companies that have interests in common with Facebook could restrict their offer to those cultural products that are among the favorites of the standard category to which the user and their contacts/friends have been assigned. The risk is therefore that individual intellectual horizons will progressively narrow. If people in social networks encounter only cultural products that reflect the preferences of the ideal type to which one belongs according to contemporary marketing, the more predictable result would be a gradual desertification of the cultural life of individuals no longer able to encounter what is unusual, unexpected, and surprising.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p>Some of the dynamics behind these narrowing horizons, however, are typical of social networks based on ‘small worlds’ – for example, those networks which consist of small groups densely connected to each other. (The model of the 'small world network' was introduced in 1988 by Duncan Watts and Steve Strogatz.)<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> As explained by Clay Shirky, these networks act as amplifiers and filters of information.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> In other words, because the information is spread from friends (or contacts) to <em>friends of friends</em>, one ends up stagnating in a flow of information filtered and amplified by one's contacts. With a closer look, everything that is not checked and highlighted in one's own small world is destined to go unnoticed.</p>
<p>To sum up, the ideology underlying frictionless sharing is based on the suppression of all criticism (whether aesthetic, social, or political) and the consequent reduction of users into 'robots without a soul' who have the unique function of consuming and therefore producing statistics, which are in turn fed to software algorithms capable of transforming them into new stereotypes and abstract consumption models. <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>Similar concerns animate Eli Pariser's reflections on 'filter bubbles', a concept which represents the universe of information that Google, Facebook, etc. specifically prearrange for each of us. This U.S. activist effectively explains how the algorithms underlying the new conception of the net are increasingly tightening the circle around our preferences and our desires, to the point that when browsing the internet, it is unlikely that we will encounter something that has not been specifically designed for us. This trend provokes great alarm, because it ends up affecting our ability to choose how we want to spend our life.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<h2 id="right-to-say-no">Right to Say No</h2>
<p>From the perspective of cultural critique, the problematic relationship of humans with machines that have the capacity to filter cultural objects has been extensively addressed in the past century and is at the center of deep reflections on freedom by Vilém Flusser. The Bohemian thinker (whose horizon of observation is the diffusion of the first personal computers in the 80s) observes with great concern the consolidation of a tendency to escape the responsibility of critical consciousness and instead delegate every decision-making process to machines. This passage threatens to deprive humans of the critical role of <em>essences who make decisions</em>, <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> which spells 'the end of freedom'<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>. It is also not possible to consider it an expression of freedom when deciding whether to press one key or another; instead, the decision is based on prescripts and, ultimately, is a 'programmed freedom' (a choice between prescribed possibilities). <a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> The escape offered by Flusser is a utopian telematic society in which it becomes possible to delegate the production of information to apparatuses in order to be free to devote oneself to the criticism of the information automatically produced by them.</p>
<p>Flusser perfectly anticipated the current scenario when he imagined that in the near future, machines would be calibrated according to informatic criteria (algorithms) to perform (automatically) the function of criticism as well. However, it should be underscored that according to Flusser, the apparatuses – which become ‘critical machines’ – assume the function of determining which redundant information (kitsch, gossip, etc.) requires filtering (discarding) and which informative data to let go , while it is clear that the kinds of experience that prelude the frictionless sharing philosophy are quite the opposite: redundant information is not discarded but heavily transmitted over digital networks. The Facebook timeline, for example, proposes the experience of continuously redundant content, that is, content that is endlessly repeated to induce users to conform to them. Moreover, as noted by Bob Hanke, '[s]ince Flusser's time, we have seen the rise of a myth of user agency and interactivity, the digital lock-down of culture, and the enclosure of the information commons.'<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a></p>
<p>Beyond this aspect, Flusser believes that even in the case of automatic apparatuses (which have become automatic critics), human beings retain a 'veto right'. The right to say 'no' ( Recht zum Neinsagen ) represents, in the Flusserian view, the negative decision ( negative Entscheidung ) that we call ‘freedom’. Ultimately, for Flusser, telematics is a technology of freedom because, while it frees us from having to make decisions, at the same time it opens the doors to the fundamental freedom whereby we can say no to telematics itself. <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p>
<h2 id="freedom-to-program">Freedom to Program</h2>
<p>Flusser's interrogation into the opportunity to reject telematics and reassert human intelligence over artificial ones is particularly challenging in an era such as the present one, in which we are all struggling to affirm our freedom ('positive freedom') to control the fate of information (especially information about ourselves), while our agency is restricted by 'the “black boxing” of technologies, their networks, and intellectual property regimes'.<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> For Flusser, to avoid being programmed by the apparatuses, it is necessary to devote oneself to their reconfiguration and programming. In other words it is necessary to say, 'I want to have my program so that I won’t be subject to anyone else’s'. In a society dominated by 'unidirectional media' (TV, radio, etc.)<em>,</em>senders (broadcasters) possess the programs, and we are possessed by them; hence, the need to dispossess and socialize programs emerges. However, in a fully realized**information society, that is, a society in which centralized senders have been overcome, it would no longer make sense to speak in terms of dispossession but rather in terms of dialogical programming. Therefore, beyond using 'one’s own program', it would be more appropriate to apply the formula 'programs of others'. In a telematic society, as Flusser points out, there is no longer the need to possess one’s own program to reduce the fear of succumbing to someone else’s program. What is more fundamental is having the 'programs of others' in order to edit (remix) them and subsequently offer them back.<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
<p>In this regard, the popular media theorist Douglas Rushkoff, in his decalogue <em>Program or Be Programmed</em>, writes that '[p]rogramming is the sweet spot, the high leverage point in a digital society. If we don’t learn to program, we risk being programmed ourselves.' <a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a> The media theorist notes that if in an earlier time there were no differences between using a computer and programming it, today we are confronted with a totally different scenario, and the goal of those who determine the development of software and hardware seems to be making interfaces increasingly stratified (below a ‘user friendly’ interface, the logic of functioning is made inaccessible and is in fact hidden under different levels of complexity). 'The easy command-line interface (where you just type a word telling the machine what you want it to do) was replaced with clicking and dragging and pointing and watching.' <a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> Put simply, the computational medium has been made more opaque, thus more similar to television. Interfaces are becoming ‘user friendly’, but the internal functioning of machines is hidden in the background. <a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> The most noteworthy examples are search engines, which billions of people use to filter information and access news. These are interfaces with the highest degree of opacity – indeed no one is able to determine exactly what algorithms they use to function. <a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> This fact does not diminish their popularity, thanks to the misperception that the results they offer are ‘objective’ and not determined by user profiling, geographical position, interests of advertisers, etc., and they are growing at accelerated rates. <a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></p>
<p>The most immediate consequence of the tendency to blur interfaces is that we are learning the features that our computers offer us but not the operations that we could make them perform. <a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> In other words, we are satisfied with following the paths drawn for us by others and do not try to forge our own way.</p>
<p>According to Rushkoff, despite the programming tools at our disposal, we continue to think in terms of the writing potential of the medium. Therefore we feel proud and satisfied enough by making a web page or filling in our profile information page on some social network site. We continue to be unaware of the biases of the programs we use, as well as of the ways in which they circumscribe our 'newfound authorship' within their predetermined agendas. <a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> Our interactivity is clearly limited when the modalities of interaction have been planned in advance. Machines are then animated, as Flusser would describe it, by technologies that represent 'black boxes' known only to those who program (and whose good faith we are forced to accept on trust). <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> As Rushkoff writes:</p>
<blockquote>
<p>Digital technologies are [...] not just objects, but systems embedded with purpose. They act with intention. If we don’t know how they work, we won’t even know what they want. The less involved and aware we are of the way our technologies are programmed and program themselves, the more narrow our choices will become. <a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a></p>
</blockquote>
<p>Rushkoff arrives at a place similar to Flusser's: for both is it clear that 'the less we will be able to envision alternatives to the pathways described by our programs […] the more our lives and experiences will be dictated by their biases'. <a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a> Moreover, as emphasized by Manovich, in a world dominated by software, it is not our skills alone that have a decisive influence – it is important also to take into consideration what software tools are used. In fact, what we can do with a single digital file changes dramatically depending on the software we have access to. <a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a></p>
<p>Flusser believed that the individual human being can do nothing to reverse a situation in which we all risk being programmed by apparatuses. However, 'society as a whole' or as a 'collective brain' has for Flusser a greater competence than apparatuses that, though boasting great speed and infallible memories, remain ‘idiotic’. The media philosopher assumed therefore that it is not individual 'functionaries' <a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> and receivers, but society as collective brain that has the capacity to regain control of apparatuses and make judgments about when desirable (informative) situations have been created.</p>
<p>To avoid being programmed by apparatuses it is necessary that society as a whole is dedicated to the dialogic reconfiguration and programming of apparatuses. <a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> But one should never forget, as Flusser warns, that this is not solely a technical but also a political question, <a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> and therefore a general agreement about reconfiguration and programming must become widespread. In particular, users should be given the opportunity to personally intervene in the configuration of software platforms they interact with, so that they can decide for themselves what to share and possibly with whom; on which information they want software to make suggestions; and for which cultural objects the filters' operation is considered useful and which they would prefer to rely on serendipity, that is, random and unexpected encounters. Ultimately, the rise of a strong political consensus regarding the inviolability of the right of users is necessary in order to decide (and to be able to modify at any time the decisions previously taken) whether and to what extent users want to rely on the automatic criticism of machines, and whether and to what extent they want to act as critics, thus as information selectors, instead.</p>
<p>It is also clear that in order for these freedoms to be effective, it is essential to overcome and leave behind the black box model that makes companies such as Google and Facebook prosper. The main condition for the development of a collective dialogue on reconfiguration and programming is in fact the transparency of the apparatuses with which one interacts. Therefore any idea of re-appropriating the role of the critic is condemned to remain an illusion until it is possible again to look inside our apparatuses, to understand how they work, and to share such knowledge. <a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a></p>
<h2 id="references" class="references">References</h2>
<p>Becker, Konrad and Felix Stalder (eds). Deep Search: The Politics of Search beyond Google, Innsbruck: Studienverlag, 2009.</p>
<p>Campanelli, Vito. InfoWar: La battaglia per il controllo e la libertà della rete , Milano: Egea, 2013.</p>
<p>Castelluccia, Claude. 'Behavioural Tracking on the Internet: A Technical Perspective', in Serge Gutwirth, Ronald Leenes, Paul De Hert, and Yves Poullet (eds) European Data Protection: In Good Health? , London and New York: Springer, 2012: pp. 21-35.</p>
<p>Doctorow, Cory. 'The Coming Civil War over General Purpose Computing', BoingBoing , August 2012, http://boingboing.net/2012/08/23/civilwar.html.</p>
<p>Flusser, Vilém. Into the Universe of Technical Images , trans. Nancy Ann Roth, intro. Mark Poster, Minneapolis and London: University of Minnesota Press, 2011 (1985).</p>
<p>_____. The Shape of Things: A Philosophy of Design, trans. Carl Hanser, London: Reaktion, 1999 (1993).</p>
<p>Galloway, Alexander R. The Interface Effect , Cambridge: Polity, 2012.</p>
<p>Hanke, Bob. 'Vilém Flusser's Digital Galaxy', International Journal of Communication (January, 2012): 25-35.</p>
<p>Manovich, Lev. 'There is Only Software', 28 April 2011, http://www.manovich.net/DOCS/Manovich.there_is_only_software.pdf.</p>
<p>Morozov, Evgeny. 'The Death of the Cyberflâneur', The New York Times , 5 February 2012.</p>
<p>Pariser, Eli. The Filter Bubble: What The Internet Is Hiding From You , London and New York: Penguin, 2011.</p>
<p>Rushkoff, Douglas. Program Or be Programmed: Ten Commands for a Digital Age , New York: ORbooks, 2010.</p>
<p>Shayon, Sheila. 'Facebook Unveils Timeline for ‘Friction-less’ Serendipity', 22 September 2011, http://www.brandchannel.com/home/post/2011/09/22/Facebook-f8-Timeline-Announcement.aspx.</p>
<p>Shirky, Clay. Here Comes Everybody: How Change Happens when People Come Together, London and New York: Penguin, 2008.</p>
<p>Vaidhyanathan, Siva. The Googlization of Everything (And Why We Should Worry) , Los Angeles: University of California Press, 2011.</p>
<p>Watts, Duncan. Small Worlds: The Dynamics of Networks between Order and Randomness , Princeton: Princeton University Press, 1999.</p>
<p>_____. Six Degrees: The Science of a Connected Age , New York: Norton, 2003.</p>
<h2 id="notes" class="notes">Notes</h2>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Zuckerberg's expression was the following: 'real-time serendipity in a friction-less experience'. See Sheila Shayon, 'Facebook Unveils Timeline for “Friction-less” Serendipity', 22 September 2011, http://www.brandchannel.com/home/post/2011/09/22/Facebook-f8-Timeline-Announcement.aspx.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Vito Campanelli, <em>InfoWar: La battaglia per il controllo e la liberta della rete</em>, Milano: Egea, 2013, p. 115.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Evgeny Morozov, 'The Death of the Cyberflâneur', <em>The New York Times</em>, 5 February 2012, p. 6.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>On the project's page is written: 'organizing the things you need to know and freeing you up to focus on what’s important to you'. Retrieved at: http://www.google.it/landing/now.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>On the state of art of profiling systems, see Claude Castelluccia, 'Behavioural Tracking on the Internet: A Technical Perspective', in Serge Gutwirth, Ronald Leenes, Paul De Hert, Yves Poullet (eds) <em>European Data Protection: In Good Health?</em>, London and New York: Springer, 2012, pp. 21-35.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Campanelli, <em>InfoWar</em>, pp. 115-116.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>See Duncan Watts, <em>Small Worlds: The Dynamics of Networks between Order and Randomness</em>, Princeton: Princeton University Press, 1999. See also: Duncan Watts, <em>Six Degrees: The Science of a Connected Age</em>, New York: Norton, 2003.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Clay Shirky, <em>Here Comes Everybody: How Change Happens when People Come Together,</em> London and New York: Penguin, 2008.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Campanelli, <em>InfoWar</em>, p. 117.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Eli Pariser, <em>The Filter Bubble: What The Internet Is Hiding From You</em>, London and New York: Penguin, 2011.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>It's useful to remember that for Flusser 'freedom is essentially the difference between that which is redundant and that which is actually information, and the free person is the one who is competent to decide'. Vilém Flusser, <em>Ins Universum der technischen Bilder</em>, Göttingen: European Photography, 1985, trans. <em>Into the Universe of Technical Images</em>, Minneapolis and London: University of Minnesota Press, 2011, p. 111.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Flusser, <em>Into the Universe of Technical Images</em>, p. 119.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Vilém Flusser, <em>The Shape of Things: A Philosophy o</em> f Design , trans. Carl Hans, London: Reaktion, 1999 (1993), pp. 92-93.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>Bob Hanke, 'Vilém Flusser's Digital Galaxy', International Journal of Communication (January, 2012): 25.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Flusser, <em>Into the Universe</em>, p. 122.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>Hanke, 'Vilém Flusser's Digital Galaxy': 25.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>Flusser, <em>Into the Universe</em>, p. 155.<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>Douglas Rushkoff, <em>Program or Be Programmed: Ten Commands for a Digital Age</em>, New York: ORbooks, 2010, p. 133.<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>Rushkoff, <em>Program or Be Programmed</em>, p. 135.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>On the unworkable nature of all interfaces (windows, doors, screens, etc.), see Alexander R. Galloway, <em>The Interface Effect</em>, Cambridge: Polity, 2012.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p>For a critical analysis of the role of search engines in current society and, in particular, of their ability to affect access to culture and information, see Konrad Becker and Felix Stalder (eds) <em>Deep Search: The Politics of Search beyond Google</em>, Innsbruck: Studienverlag, 2009, and Siva Vaidhyanathan, <em>The Googlization of Everything (And Why We Should Worry)</em>, Los Angeles: University of California Press, 2011.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>The statistical analysis company comScore recently documented the breaking of the 20 billion searches per month roof, which happened in the U.S. market in March 2013. The research is available online at http://www.comscore.com/Insights/Press_Releases/2013/4/comScore_Releases_March_2013_U.S._Search_Engine_Rankings.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>Rushkoff, <em>Program or Be Programmed</em>, p. 138.<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p>Rushkoff, <em>Program or Be Programmed</em>, pp. 139-140.<a href="#fnref24">↩</a></p></li>
<li id="fn25"><p>Rushkoff, <em>Program or Be Programmed</em>, p. 141.<a href="#fnref25">↩</a></p></li>
<li id="fn26"><p>Rushkoff, <em>Program or Be Programmed</em>, pp. 142-143.<a href="#fnref26">↩</a></p></li>
<li id="fn27"><p>Rushkoff, <em>Program or Be Programmed</em>, p. 143.<a href="#fnref27">↩</a></p></li>
<li id="fn28"><p>Lev Manovich, 'There is Only Software', 28 April 2011, http://www.manovich.net/DOCS/Manovich.there_is_only_software.pdf.<a href="#fnref28">↩</a></p></li>
<li id="fn29"><p>Flusser writes: 'most apparatuses are not so completely automatic that they can get along without human intervention. They need functionaries. In this way, the original terms human and apparatus are reversed, and human beings operate as a function of the apparatus.' Flusser, <em>Into the Universe</em>, p. 74.<a href="#fnref29">↩</a></p></li>
<li id="fn30"><p>Flusser, <em>Into the Universe</em>, pp. 73-77.<a href="#fnref30">↩</a></p></li>
<li id="fn31"><p>Flusser, <em>Into the Universe</em>, p. 76.<a href="#fnref31">↩</a></p></li>
<li id="fn32"><p>These considerations point to the more general issue of general purpose computers, a complex subject that has many criticalities addressed by the digital rights activist Cory Doctorow, see Cory Doctorow, 'The Coming Civil War over General Purpose Computing', <em>BoingBoing</em>, August 2012, http://boingboing.net/2012/08/23/civilwar.html.<a href="#fnref32">↩</a></p></li>
</ol>
</div>
</body>
</html>
