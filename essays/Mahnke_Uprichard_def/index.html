<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
</head>
<body>
<p>Algorithming the Algorithm</p>
<p>Martina Mahnke and Emma Uprichard</p>
<p>#hello</p>
<p>Imagine sailing across the ocean. The sun is shining, vastness all around you. And suddenly [BOOM] you’ve hit an invisible wall. Welcome to the Truman Show! Ever since Eli Pariser published his thoughts on a potential filter bubble,<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup> this movie scenario seems to have become reality, just with slight changes: it’s not the ocean, it’s the internet we’re talking about, and it’s not a TV show producer, but algorithms that constitute a sort of invisible wall.<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup> Building on this assumption, most research is trying to ‘tame the algorithmic tiger’.<sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup> While this is a valuable and often inspiring approach, we would like to emphasize another side to the algorithmic everyday life. We argue that algorithms can instigate and facilitate imagination, creativity, and frivolity, while saying something that is simultaneously old and new, always almost repeating what was before but never quite returning. We show this by threading together stimulating quotes and screenshots from Google’s autocomplete algorithms. In doing so, <em>we invite the reader to re-explore Google’s autocomplete algorithms in a creative, playful, and reflexive way</em>, thereby rendering more visible some of the excitement and frivolity that comes from being and becoming part of the riddling rhythm of the algorithmic everyday life.</p>
<p>#warning</p>
<p>We’ve adopted an alternative textual style, which may annoy and confuse some readers, though hopefully also amuse and intrigue others. Therefore we keep this discussion reasonably short and deliberately provocative. However, we don’t want to confuse or provoke so that nothing is heard. Our purpose is not to mess with the importance of academic prose for the sake of it. Instead, what we do is experimental and reflects some of the unexpected landings, spontaneity, and at times insanity of our algorithmically-linked travels into (new and old) media spaces.</p>
<p>#searching</p>
<p>[IMG1]</p>
<p>Fig. 1: I Google, therefore I am</p>
<p>Search engines in general, and Google in particular, have become entry points to the internet. We use Google to verify information, to verify credibility, to verify existence. Google seems to answer all of our questions. ‘I google, therefore I am’? René Descartes tried to prove existence by radically doubting his own; now visibility through Google is used as proof. Google seems to be all-knowing and omnipresent. Is Google the new God? The ‘Church of Google’ seeks proof through scientific reasoning and concludes, ‘We at the Church of Google believe the search engine Google is the closest humankind has ever come to directly experiencing an actual God’.<sup><a href="#fn4" class="footnoteRef" id="fnref4">4</a></sup> ‘To google’ has become today’s synonym for searching the web, even though official language committees disfavor this trend. Sometimes it’s hard to imagine what life would be without Google. What would we do? Use another search engine? Not care about information on the internet?</p>
<p>Trying to answer these, albeit hypothetical, questions leads us to the conclusion that Google is more than just a tool; it is a modern myth. But let’s not forget, Google is still also ‘just’ a search engine. The development of search engines can be divided into three periods from a marketing perspective:<sup><a href="#fn5" class="footnoteRef" id="fnref5">5</a></sup> (1) the period of ‘technological entrepreneurs’ from 1994 until 1997, (2) the period of ‘portals and vertical integration’ from 1997 until 2001, and (3) the period of ‘syndication and consolidation’ starting in 2002, which still persists. Google introduced its radically new search algorithm in the second period in 1998.<sup><a href="#fn6" class="footnoteRef" id="fnref6">6</a></sup> Google’s autocomplete function was launched 1 April 2009, and it’s this autocomplete algorithm that we want to focus on in our discussion.</p>
<p>Our aim is not a detailed critique of what has already been written on the subject.<sup><a href="#fn7" class="footnoteRef" id="fnref7">7</a></sup> Instead, we want to offer an alternative way of thinking about search, to reflect some of ‘jagged-jixie’ involved when we casually surf, explore, and search the web, when we find (not) new material, (not) new friends, and (not) new colleagues – all the while speaking through and with the algorithm. We aim to contribute to current discussions about the social implications of algorithms. After all, algorithms are quintessentially social – they are socially constructed and take on meanings that are variable across time and space. Algorithms are full of ethical and political issues; they (dis)connect the (inter)connected, and they are let loose by the very few with the power and technical expertise to do so. So yes, we know and agree that algorithms are powerful and governing, as so much of the existing literature argues. However, algorithms can also instigate imagination, creativity, and frivolity, if we allow them to. And all of a sudden we might learn: their power is mutable, once we start playing around.</p>
<p>#perspectives</p>
<p>From a mathematical perspective, algorithms have been around for centuries. As Charbert summarizes in simple terms,<sup><a href="#fn8" class="footnoteRef" id="fnref8">8</a></sup> algorithms are recipes or step-by-step instructions. Algorithms were used by the Babylonians for deciding points of law, as well as by Latin teachers for teaching grammar. Today’s computer algorithms are more complex processes, developed and translated into machine language by the fields of machine learning and computer intelligence. In 1950 in his ‘game of imitation’, Alan Turing, one of the pioneers in this field, raised this question for the first time: what if machines can think?<sup><a href="#fn9" class="footnoteRef" id="fnref9">9</a></sup> He theorized that if machines can imitate the communication of a human, they could be considered just as intelligent. While back then this speculation was a <em>gedankenexperiment</em>, it has now almost, depending on the perspective, become true. An ordinary user is not able to distinguish between machinery and human input anymore. Data journalism – semi-automated production of text out of statistical data – has taken over the simple text-producing routines of journalists. This ‘algorithmic turn’ can be seen in various areas in society. Even though we are not entirely determined by technology, we are definitely living in a world that is strongly driven by computational algorithmic technology. This development can most clearly be seen in the area of social media. While social media producers started with vague ideas and deficient business models, they now are allocated a great part of the internet, influencing our definition of what information is and how it should be distributed.</p>
<p>Computational processes are carried out through algorithms. The algorithms used in search engines are still a list of instructions, but they are not as strict as ‘either-or’ rules; most of the time, they fall into the category of intelligent algorithms, which means they learn and change over time. Typically, authors tend to represent the issue of algorithms as a question of control: who has control over what and whom? This is a classic question of sovereignty, which introduces the notion of hierarchy. Thought of this way, algorithms can be frightening; they are surveilling, governing, and controlling. While exploring algorithms and Google’s search algorithm from the perspective of power regimes is valuable and inspiring, we’d like to highlight another side of our algorithmic everyday life, namely what it means ‘to find’. To discover something that may or may not have been searched. <em>Search and find need one another</em>; they come together and make castles in the digital sand, building layers upon layers of new configurations and algorithmic word towers. What we have found is that algorithms can say something that is simultaneously old and new, always almost repeating what was before, but never quite returning. Fast forward. Rewind. Google’s autocomplete search algorithm is another way of searching and finding, re-imagining, creating, and playing with what is there already in the present and what will be in the future. In other words, autocomplete instigates and facilitates imagination, creativity, and frivolity.</p>
<p>#whatalgorithmsdo</p>
<p>Through their rule-based simplicity, algorithms enable a space, place, and time to re-do the old and new simultaneously, nonlinearly, in an awe-inspiring magical kaleidoscope. As Waldrop sums up beautifully:</p>
<p>As we begin to understand complex systems, we begin to understand that we’re part of an ever-changing, interlocking, non-linear, kaleidoscopic world. […] The elements always stay the same, yet they’re always rearranging themselves. So it’s like a kaleidoscope: the world is a matter of patterns that change [and have continuity], that partly repeat, but never quite repeat, that are always new and different.<sup><a href="#fn10" class="footnoteRef" id="fnref10">10</a></sup></p>
<p>Repeating and rearranging, threading and weaving, knitting the crochet macabre macramé world of the web. Searching and re-searching. Re-finding and de-re-finding. The new and the old. Growing and changing and not changing together. The invariable, invariant variety of patterns in the Googleland of the present. Past becomes the future of futures, time and time and time again. Again. Repeat. The dynamics of the algorithmic ‘live’ search engine ensure the patterned repetition and ever-morphing world.</p>
<p>Inputting, storing, processing, and outputting are the functions of our algorithms; this is all they can (currently) do. We embrace the algorithm and its rules-based logarithms; they are simple. If you are an algorithm, there is no ambiguity. You need to know exactly which step, which function, which action will be performed. Humans, on the other hand, introduce ambiguity, fuzziness, and confusion. The unambiguous algorithmic world combines with the lone surfer, who constantly flits from one thought to the next, confusing paths, laying new stones for those who follow, kicking up dust, masquerading as the present, multiplying the future, and re-clicking a new and emergent present. Masquerading at a ball, like a harlequin clown, juggling with hyperlinks and net-edges, working and erasing time all the time.</p>
<p>It’s not just fun and play. Algorithms have social implications. Donald Knuth,<sup><a href="#fn11" class="footnoteRef" id="fnref11">11</a></sup> Thomas Corman et al.,<sup><a href="#fn12" class="footnoteRef" id="fnref12">12</a></sup> Steven Skiena,<sup><a href="#fn13" class="footnoteRef" id="fnref13">13</a></sup> and many authors have shown that the relationship between algorithms and the social is complicated. But only sort of. There are so many ways to sort and search the social. Clustering and classifying. Machines learning patterns. Sorting this thing from that, in and out. This box, not that one. With those and without these. Sorting what cannot be sorted easily. Behind the ‘black box’ of search algorithms lies the need to sort out in order to search. Indeed, a search algorithm mostly comes down to how we measure and define difference and similarity, distance and proximity, how something is measured to be ‘far’ or ‘near’. Scaling up and scaling down. Ordering disorder. Always and everywhere. This or that. In short: sorting is all about <em>matching</em> and <em>ranking</em>. Two items are matched against each other and displayed in a ranked (dis)order.</p>
<p>#diggingintothealgorithm</p>
<p>In order to dig into Google’s autocomplete algorithm a little further, we took screenshots and combined them with citations. We joined co-authored emails and internet searches. To and from, as we both searched for the search. Piecing together the incoherent. Reflecting the process and the (con)text. Tracing and tracking, snatching and clipping. In Denmark and the U.K. The data reflects the autoethnographic approach taken. Participating and observing. Immersing. Taking notes. Memos. Comments. Getting lost and coming back. In order to think, reflect, and write. Understanding the internet as a <em>digging place</em> for archaeological excavation. And using the internet to dig through the autocompleted links.</p>
<p>&gt;Start [enter]</p>
<p>[IMG 2]</p>
<p>Fig. 2: Same but different</p>
<p>#thinkingmachines</p>
<p>What’s the difference between human and machine? Is there a difference? Daniel Dennett [I’m feeling lucky ENTER] argues: ‘It is that special inner light, that private way that is with you that nobody else can share, something that is forever outside the bounds of computer science.’<sup><a href="#fn14" class="footnoteRef" id="fnref14">14</a></sup> Boden noted in 1985: ‘Artificial intelligence (AI) was conceived in the mid-1940s […]. Since then, it has had some notable successes, enabling computers to perform – albeit in a very limited way – some of the tasks normally done by our minds.’<sup><a href="#fn15" class="footnoteRef" id="fnref15">15</a></sup> In the end isn’t AI just a copycat copying what’s already there? Trying to reach the original, just never quite getting there?</p>
<p>As can be seen in Figure 2, a quick search for ‘Can machines think?’, typed in a sloppy everyday way, not caring about upper and lower cases and exclamation marks, in Denmark and the U.K., produces slightly similar, but still different autocomplete terms. Yet, in Figure 3 we notice it is not only that the autocompleted terms differ across countries, but also across time. That is to say, the same search terms used just 11 minutes apart make visible a different set of terms – some the same and some new. In contrast, the same search terms two days apart have no visible effect. While these outcomes raise concerns, they also trigger a whole other possible set of reactions. Indeed, it is tempting to immediately question the censoring and governing actions of the algorithms, which delimit and determine the outcomes of our searches. We want to suggest that it is precisely in the similarities and differences of Google’s autocomplete that we are offered alternative ways of ‘playing’ with and against Google itself. Moreover, it is in making visible these different and similar affordances that Google’s autocomplete algorithm becomes both a site and a mechanism whereby <em>imagination, creativity, and frivolity</em> produce accessibility. **</p>
<p>[IMG 3]</p>
<p>Fig. 3: Difference in time and space</p>
<p>[IMG 4]</p>
<p>Fig. 4: Growing an ‘imagination tree’</p>
<p>&gt; End [stop]</p>
<p>#imagination</p>
<p>Imagination is a way of seeing, knowing, and learning; it asks us to look beyond what we take for granted.<sup><a href="#fn16" class="footnoteRef" id="fnref16">16</a></sup> Imagination premises openness. Imagination builds on top of the known. Imagination resides ‘where perception, memory, idea generation, emotion, metaphor, and no doubt other labeled features of our lives, intersect and interact’.<sup><a href="#fn17" class="footnoteRef" id="fnref17">17</a></sup> The collected data shows that the actual list of words varies across time and space. The differences and similarities matter as much as they do not matter and simply echo Google’s idea of relevance, trying to sense and predict the virtual waves of the digital seas between man and machine. Imagining a world without Google is now like remembering a world without computers. It is not the world we live in. What is for sure: other things will take over, replacing Google in a time to come. At the moment, however, our searches begin and end with Google's stamp.</p>
<p>In Figure 4, for example, Google’s autocomplete algorithm has started a new, imaginative story for us, where typing ‘algo’ with algorithm in mind led us to ‘algonquin’, a hotel. Autocomplete allows us to see, use, and re-invent. We might start nurturing an ‘imagination tree’. Any number of stories might stem from Google’s autocomplete terms when used and re-imagined creatively, and the stories might never have been imagined without Google giving us some words to play with in the first place.</p>
<p>But yes, Google also censors the unsearched. However, it’s inevitable that we never know what we cannot know. Therefore, algorithms do have the capacity to reinforce what we already know; this can also mean reinforcing negative discourses. Baker and Potts, for example, suggest that racist, homophobic, and sexist stereotypes are reinforced through visible autocomplete words.<sup><a href="#fn18" class="footnoteRef" id="fnref18">18</a></sup> Imagination, however, may help to balance this out and let us find nuggets of word-worlds that we may not have previously considered. How can a search engine know what we do not know and may not want to know? In our data material, we find that Google U.K. tends to make fewer words visible than Google Denmark. Is it really Google, or, since the query is based on personal search, does it rather have something to do with our own search behavior? Are these really ‘missing’ words? Are they even hidden? Or just visible somewhere else? Then how might we find them? Did no one ask for them? Does no one care? The irony is that to answer these questions, we are pulled into the mystery of Google, once again. Searching for answers while Google has us trapped. Searching for the search to find the answer to the search sucks us back into an almost never-ending quest to find the answers to the questions we have never searched. However, this search also allows us to maintain a sense of creativity.</p>
<p>#creativity</p>
<p>Creativity is process. Creativity constitutes. To view Google’s autocomplete creatively can help us learn and think in new ways. The initial conditions of our googled ideas vary across time and space, depending on which words float to the top (Figure 3). As Alexander suggests, ‘there is a deep and important underlying structural correspondence between the pattern of a problem and the process of designing a physical form which answers that problem’.<sup><a href="#fn19" class="footnoteRef" id="fnref19">19</a></sup> And so too is the case here: the underlying structure of Google’s autocomplete algorithm presents its own problem of form. That is to say, the physical form of the algorithm lies in part in its effects. Conversely these effects shape the algorithm, like a sculptor shape-shifting the clay he or she shapes, always morphing dialectically together, yet always with a little more control than the clay itself, which only becomes animated through the hands of the sculptor. Google’s search enables us to find what we want to find, but those discoveries are also shaped by a massively abnormal curve that is constantly contorted by unpredictable word lists.</p>
<p>In turn, therefore, there is potential for maximizing our crowdsourcing capacity to shape Google’s autocomplete in our favor. For example, what would happen if we collectively ceased to use Google for just one day? Alternatively, what if billions of users searched ‘capitalism needs to change’ or ‘racism must be punished’ at the same time? Given the recursivity built into Google’s autocomplete algorithm, which is clearly sensitive to time and space, perhaps there is more we can do to change normative attitudes than we may have yet realized. Perhaps the capacity to act collectively towards a common good is by actively interacting with Google’s autocomplete in ways that are similar to activist movements. If Google can <em>shape</em> our views, perhaps we can use Google to <em>change</em> our views?</p>
<p>#frivolity</p>
<p>Frivolity is considered a harmless, unproductive activity outside the more structured activity of play. Huizinga (1950) in Homo Ludens notes that play is a 'free activity standing quite consciously outside of “ordinary” life as not being serious, but at the same time absorbing the player intensely and utterly.' Frivolity, on the other hand, is more transitory and generally without rules or order.^^ <sup><a href="#fn20" class="footnoteRef" id="fnref20">20</a></sup></p>
<p>In <em>The Archaeology of the Frivolous’,</em> Jacques Derrida argues, among other things, that frivolity is always seen as ‘bad’ and ‘dangerous’ and that we can use it to disturb and deconstruct views that are taken for granted.<sup><a href="#fn21" class="footnoteRef" id="fnref21">21</a></sup> Indeed, one of the key points we want to make is that search algorithms need to be carefully looked at and looked after. We need to make sure that we know who is doing the looking and looking after. However, we want to emphasize that even though search algorithms tend to take on a life of their own once they are produced, put into action, and acted upon, they are no different to any other social construct. The power they are assigned, and the independence they are seen to take on, are both socially constructed. Their power is made to ‘hold’ in much the same way that statistical measures of significance are ‘made’ to hold through the social practices and institutions that reinstate them, as Desrosières argues in his excellent book, <em>The Politics of Large Numbers</em>.<sup><a href="#fn22" class="footnoteRef" id="fnref22">22</a></sup> Indeed, we go so far as to say that search algorithms are constructions just as are objects in science. As Collins and Pinch point out in their classic book, <em>The Golem: What You Should Know About Science,</em> <sup><a href="#fn23" class="footnoteRef" id="fnref23">23</a></sup> it is not science that is ‘good’ or ‘bad’. Rather, what is done with and to the world in the name of ‘science’ is what can be judged as good or bad.</p>
<p>Perhaps, as we have suggested throughout, it is not Google’s search algorithm that is problematic, despite its power. Instead, it is perhaps the things that we as individuals, and collectively as governments, may (or may not) do (and don’t do) with Google which are problematic. In itself, the autocomplete algorithm may be seen as a Golem, following Collins and Pinch's logic. A mythological creature. A bit daft. Silly. Playful. Not good or bad. Just searching. Waiting for instructions on what to search for. In turn, therefore, we suggest that we can put Google’s search algorithm to frivolous use to disrupt and disturb the ordinary words that inscribe and shape our world. Using Google’s autocomplete to resist and confuse. After all, for every ‘search’, there needs to be a ‘find’ to discover something that may or may not have been searched. Search and find need one another. <sup><a href="#fn24" class="footnoteRef" id="fnref24">24</a></sup> Browsing and laughing through hyperlinks, giggling away at the joy of discovering and uncovering and recovering new spaces, new worlds, the unexpected. Using Google frivolously to make up new algorithmic fables, new tales, and new futures. Daily, mundanely. Juggling with hyperlinks and autocorrect. Everyday. Every day. ‘Taming the algorithmic tiger’ with a smile, with frivolity, creativity, and imaginary rhythms.</p>
<p>#lessonstolearn</p>
<p>‘Ceci n’est pas une pipe’ – ‘this is not a pipe’. Magritte shows us that things are not always what they appear to be. We live in a world in which we rely heavily on information on the one hand, but, on the other, cannot validate it anymore through our own experiences. Hence, we gather information through various intermediaries such as Google’s autocomplete algorithms. While in the era of print these intermediaries seemed definite and somehow understandable, this is not the case anymore. Let’s face it: the internet – once thought of as a free information space – has become quite chaotic. Constant innovation within the area of digital technology and machine intelligence offers algorithmic answers to everyday information chaos. The concepts behind the algorithms are not easy to grasp, especially when businesses rely on the secrecy of those algorithms in order stay in business. This situation has created anxiety about what kind of information users receive. Mostly it is feared that when algorithms calculate their stream based on a set of rules that nobody knows, information becomes one-sided. We don’t think this outcome will necessarily be the case: search algorithms are not solely based on the algorithm itself, but also on us as users and our search-and-find behavior. <em>Let’s start accepting this state of things and stop surrendering</em>.</p>
<p>Searching the search is finding difference and similarity, change and stagnancy, always in time and space. Algorithms learn and change and repeat and remain the same too. Search algorithms govern and are visibly invisible; this much we already know. What we have done and need to do now is – <em>play creatively</em>. What Google gives us is the chance to play around. How boring would the world be if we always got the same results, whatever we did? This actually gives us the opportunity to be active. We’re not running into a wall that always stays the same no matter what. Goodbye Google Truman Show! If we understand algorithms as mirrors, we can actively take on responsibility to create our own mirror image. We have never had that chance before. We have never been that kind of ‘enabled citizen’. Let’s stop measuring Google against some sort of mythical normative standard and start to think about how we can use Google creatively.</p>
<p>And importantly, we must not ignore the fun and delight and playfulness of the search. After all, search is duality. Search allows us to make sense of the world and ourselves in that world. Search is a verb. ‘Searching’ implies that the ‘searcher’ knows something about what they are looking for. Indeed, we might say that:</p>
<p>Search      =       Being curious.</p>
<p>      =      Looking for answers.</p>
<p>      =       Looking for the unknown.</p>
<p>      =      Finding.</p>
<p>=      Finding the known.</p>
<p>      =      Finding the unknown.</p>
<p>      =      Finding questions.</p>
<p>      =      Being confused.</p>
<p>      =      Being surprised.</p>
<p>      =      Being not surprised.</p>
<p>      =      A social practice in different times but the same place.</p>
<p>      =      A social practice at the same time but in different places.</p>
<p>      =      A social practice that finds the same in different places in different times.</p>
<p>      =       Searching for searches and finding searches.</p>
<p>      =       Jack-in-a-box, always and never a surprise, predictably unpredictable.</p>
<p>#thestorybehind</p>
<p>The story started on Facebook in May 2013. Martina (author one) had set up a new Facebook page and invited friends to ‘like’ the page. Emma (author two) received such an invitation, not from Martina, as both authors didn’t know each other, but instead from another of her Facebook friends. The invitation was to ‘like’ a page called ‘Algorithmic Media Spaces’, but the next day, Emma noticed that the name of the page had changed to ‘Algorithmic Media’. Thinking that her friend had set up the page, Emma sent a silly message asking about the change of title (Figure 5). ‘Algorithmic Media’, alias Martina, replied and explained.</p>
<p>[IMG 5]</p>
<p>Fig. 5: Facebook conversation</p>
<p>This is how algorithms brought us together, just as they bring so much of our world together, and they do so across time and space, everyday (Figure 6).</p>
<p>[IMG 6]</p>
<p>Fig. 6: How the authors are related in time and space</p>
<p>#thanks</p>
<p>We’d like to thank Philipp Lenssen ( http://outer-court.com ) for kindly allowing us to use his image in the #searching section. We have titled it ‘I Google, therefore I am’. It’s not the original title. We further like to thank Miriam Rasch and René König for their editorial effort.</p>
<p>#references</p>
<p>Alexander, Christopher. <em>Note on the Synthesis of Form</em>, Cambridge, MA: Harvard University Press, 1964.</p>
<p>Baker, Paul and Amanda Potts. ‘“Why do white people have thin lips?&quot; Google and the Perpetuation of Stereotypes via Auto-Complete Search Forms’,<em>Critical Discourse Studies</em> 10.2 (2013): 187-204.</p>
<p>Bates, Rodger A. and Emily Fortner. 'The Social Construction of Frivolity', <em>The Journal of Professional and Public Sociology</em>, 5.1 (2013), http://digitalcommons.kennesaw.edu/jpps/vol5/iss1/5.</p>
<p>Beer, David. ‘Power Through the Algorithm? Participatory Web Cultures and the Technological Unconscious’, <em>New Media &amp; Society</em>11 (September 2009): 985-1002.</p>
<p>Boden, Margaret A. ‘The Social Impact of Thinking Machines’ in Tom Forester (ed.) <em>The Information Technology Revolution</em>, Oxford: Basil Blackwell, 1985, pp. 95-103.</p>
<p>Bucher, Taina. ‘Want to Be on the Top? Algorithmic Power and the Threat of Invisibility on Facebook’, <em>New Media &amp; Society</em> 14.7 (November 2012): 1164-1180.</p>
<p>Chabert, Jean-Luc. <em>A History of Algorithms</em>, Berlin, Heidelberg: Springer, 1999.</p>
<p>Collins, Harry and Trevor Pinch. <em>The Golem at Large: What You Should Know about Technology</em>, Cambridge, U.K.: Cambridge University Press, 1998.</p>
<p>Corman, Thomas H., Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. <em>Introduction to Algorithms</em>, Cambridge MA: MIT Press</p>
<p>Dennett, Daniel. ‘Can Machines Think?’ In Christof Teuscher (ed.) <em>Alan Turing: Life and Legacy of a Great Thinker</em>, Berlin: Springer, 2004</p>
<p>Derrida, Jacques. <em>The Archeology of the Frivolous</em>, trans. J. Leavey Jr., Pittsburg: Duquesne University Press, 1973.</p>
<p>Desrosières, Alain. <em>The Politics of Large Numbers: A History of Statistical Reasoning</em>, trans. Camille Naish, Cambridge, MA: Harvard University Press, 1998.</p>
<p>Egan, Kieran. <em>Imagination in Teaching and Learning: The Middle School Years</em>, Chicago: The University of Chicago Press, 1992.</p>
<p>Gillespie, Tarleton. ‘Can an Algorithm Be Wrong? Twitter Trends, the Specter of Censorship, and Our Faith in the Algorithms Around Us’, Culture Digitally Blog, 19 October 2011, http://culture digitally.org/2011/10/can-an-algorithm-be-wrong/.</p>
<p>Knuth, Donald. <em>Fundamental Algorithms,</em>vol. 1 of <em>The Art of Computer Programming</em>, Boston: Addison-Wesley, 1997 (1968).</p>
<p>_____, <em>Sorting and Searching,</em>vol. 3 of <em>The Art of Computer Programming</em>, Boston: Addison-Wesley, 1993 (1973).</p>
<p>Lake, Robert. <em>A Curriculum of Imagination in an Era of Standardization: An Imaginative Dialogue with Maxine Greene and Paulo Freire</em>, Charlotte, NC: Information Age Pub, 2013.</p>
<p>Naughton, John. 'How Algorithms Secretly Shape the Way We Behave', <em>The Guardian</em>, 15 December 2012, http://www.guardian.co.uk/technology/2012/dec/16/networker-algorithms-john-naughton.</p>
<p>Pariser, Eli.<em>The Filter Bubble</em>, London: Penguin Press, 2011.</p>
<p>Rieder, Bernhard. ‘Democratizing Search? From Critique to Society-oriented Design’, in Konrad Becker and Felix Stalder (eds) <em>Deep Search. The Politics of Search beyond Google</em>, StudienVerlag/Transaction Publishers, 2009, pp. 133-151.</p>
<p>_____. ‘Networked Control: Search Engines and the Symmetry of Confidence’, <em>International Review of Information Ethics</em> 3.1 (2005): 26-32.</p>
<p>Skiena, Steven. <em>The Algorithm Design Manual</em>, London: Springer, 2008.</p>
<p>Slavin, Kevin. 'How Algorithms Shape Our World', Ted Talks, July 2011 http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world.html.</p>
<p>Steiner, Christopher. <em>Automate This: How Algorithms Came to Rule Our World</em>, New York, NY: Portfolio/Penguin, 2012.</p>
<p>Turing, Alan. ‘Computing Machinery and Intelligence’, in: Paul A. Meyer (ed.) <em>Computer Media and Communication: A Reader</em>, Oxford: Oxford University Press, 1999, p. 37-58.</p>
<p>Van Couvering, Elizabeth Jane. <em>Search Engine Bias</em>, PhD diss., Department Media and Communications, London School of Economics and Political Science, London, U.K., 2009.</p>
<p>Waldrop, M. Mitchell. <em>Complexity: The Emerging Science at the Edge of Order and Chaos</em>, New York: Simon and Schuster, 1992.</p>
<h1 id="notes">Notes</h1>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>1 Eli Pariser, <em>The Filter Bubble,</em>London: Penguin Press, 2011.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>2 Kevin Salvin, 'How Algorithms Shape Our World', Ted Talks, July 2011, http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world.html; John Naughton, 'How Algorithms Secretly Shape the Way We Behave', <em>The Guardian,</em> 15 December 2012, http://www.guardian.co.uk/technology/2012/dec/16/networker-algorithms-john-naughton.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>David Beer, ‘Power Through the Algorithm? Participatory Web Cultures and the Technological Unconscious’, <em>New Media &amp; Society</em>11 (September 2009): 985; Taina Bucher, ‘Want to Be on the Top? Algorithmic Power and the Threat of Invisibility on Facebook’, <em>New Media &amp; Society</em>14.7 (November 2012): 1164-1180; Tarleton Gillespie, ‘Can an Algorithm Be Wrong? Twitter Trends, the Specter of Censorship, and Our Faith in the Algorithms Around Us’, Culture Digitally Blog, 19 October 2011, http://culture digitally.org/2011/10/can-an-algorithm-be-wrong/; Bernhard Rieder, ‘Democratizing Search? From Critique to Society-oriented Design’, in Konrad Becker and Felix Stalder (eds) <em>Deep Search. The Politics of Search beyond Google,</em> StudienVerlag/Transaction Publishers, 2009, pp. 133-151; Bernhard Rieder. ‘Networked Control: Search Engines and the Symmetry of Confidence’, <em>International Review of Information Ethics</em>3.1 (2005): 26-32; Christopher Steiner, <em>Automate This: How Algorithms Came to Rule Our World,</em> New York, NY: Portfolio/Penguin, 2012.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Church of Google, http://www.thechurchofgoogle.org/.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Elizabeth Jane Van Couvering, <em>Search Engine Bias</em>, PhD diss., Department Media and Communications, London School of Economics and Political Science, London, 2009.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Van Couvering, <em>Search Engine Bias</em>, p. 113.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>A great reading list is available at http://governingalgorithms.org/resources/reading-list/.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Jean-Luc Chabert, <em>A History of Algorithms,</em> Berlin, Heidelberg: Springer, 1999.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Alan Turing, ‘Computing Machinery and Intelligence’, in: Paul A. Meyer (ed.) <em>Computer Media and Communication: A Reader,</em> Oxford: Oxford University Press, 1999, pp. 37-58.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>M. Mitchell Waldrop, <em>Complexity: The Emerging Science at the Edge of Order and Chaos,</em> New York: Simon and Schuster, 1992, p. 332.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Donald Knuth, <em>Fundamental Algorithms</em>, vol. 1 of <em>The Art of Computer Programming</em>, Boston: Addison-Wesley, 1997 (1968); Donald Knuth, <em>Sorting and Searching,</em> vol. 3 of <em>The Art of Computer Programming,</em>Boston: Addison-Wesley, 1993 (1973).<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Thomas Corman et al., <em>Introduction to Algorithms,</em> Cambridge, MA: MIT Press, 2009.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Steven Skiena, <em>The Algorithm Design Manual</em>, London: Springer, 2008.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>14 Daniel Dennett, ‘Can Machines Think?’ In Christof Teuscher (ed.) <em>Alan Turing: Life and Legacy of a Great Thinker,</em>Berlin: Springer, 2004, p. 313.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>15 Margaret A. Boden, ‘The Social Impact of Thinking Machines’ in Tom Forester (ed.)<em>The Information Technology Revolution,</em>Oxford: Basil Blackwell, 1985, pp. 95-103.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>Kieran Egan, <em>Imagination in Teaching and Learning: The Middle School Years,</em> Chicago: The University of Chicago Press, 1992.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>Robert Lake, <em>A Curriculum of Imagination in an Era of Standardization: An Imaginative Dialogue with Maxine Greene and Paulo Freire,</em> Charlotte, NC: Information Age Pub, 2013.<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>Paul Baker and Amanda Potts, ‘“Why do white people have thin lips?&quot; Google and the Perpetuation of Stereotypes via Auto-Complete Search Forms’, <em>Critical Discourse Studies</em> 10.2 (2013): 187-204.<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>Christopher Alexander, <em>Note on the Synthesis of Form</em>, Cambridge, MA: Harvard University Press, 1964, p. 132.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>Rodger A. Bates and Emily Fortner, 'The Social Construction of Frivolity', <em>The Journal of Professional and Public Sociology</em>, 5.1 (2013), http://digitalcommons.kennesaw.edu/jpps/vol5/iss1/5.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p>Jacques Derrida, <em>The Archeology of the Frivolous</em>, trans. J. Leavey Jr., Pittsburg: Duquesne University Press, 1973.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>Alain Desrosières, <em>The Politics of Large Numbers: A History of Statistical Reasoning</em>, trans. Camille Naish, Cambridge, MA: Harvard University Press, 1998.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>Harry Collins and Trevor Pinch, <em>The Golem at Large: What You Should Know about Technology,</em> Cambridge, U.K.: Cambridge University Press, 1998.<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p>See #intro.<a href="#fnref24">↩</a></p></li>
</ol>
</div>
</body>
</html>
