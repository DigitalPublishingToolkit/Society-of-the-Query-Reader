##Frictionless Sharing: The Rise of Automatic Criticism

##Vito Campanelli

In recent months, revelations about the existence of the surveillance
program called PRISM and the consequent scandal (the ‘datagate’) that
has troubled the administration of U.S. President Obama has caused an
outbreak of public debate on the internet. Our awareness of programs
that allow some U.S. and European intelligence agencies to spy on
communications on the web (in addition to telephonic ones) has been
increasing. However, it should be noted that very often the worst risks
to the privacy of netizens do not come from the uncertain and
contradictory policies of Western governments, but rather from some
unexpected protagonists: the web companies that manage and organize the
online spaces within which we spend an increasingly significant part of
our days, transmitting huge amounts of data related to our personal
spheres. The most famous and popular social network, Facebook, whose
founder, Mark Zuckerberg, announced in the fall of 2011 the launch of a
new sharing philosophy called 'frictionless sharing', provides an
emblematic example of this.

The event was celebrated at Facebook F8, a conference organized by
Facebook in a rather irregular fashion (no conferences were held in
2009, 2012, and 2013) to bring together developers and companies that
make products and services that have been integrated into the social
network. During these meetings, Facebook has frequently introduced new
features. At the 2011 conference the most important novelty was the
so-called ‘timeline’, a reorganization of the interface so that the
history of users' activity is shown on their profile. Compared to such
an authentic revolution in user experience, Zuckerberg's reference
to**frictionless sharing slipped into the
background.[^1] ^^Indeed, only the most attentive
participants caught the important implications of the approach
underlying this conception.

The premise of this new philosophy of sharing is that one will no longer
need to do the work of sharing tastes and preferences. Facebook, in
fact, will register tastes automatically, thus making them available to
the contacts (friends) of each user. This spur to conformism would be
limited for the time being to media objects (movies, music, books, etc.)
published in the most popular walled garden*,* but clearly, in the near
future, nothing could stop Facebook from also recording consumption
choices and preferences relative to all the other places visited by a
user on the internet (and available technologies already allow such
monitoring).[^2] Evgeny Morozov is among those who
noticed this claim by Facebook's founder. Never tired of emphasizing the
weaknesses of internet culture, he perfectly captured that the most
interesting aspect of this feature is the ideological one: Facebook's
propaganda efforts aim to present an extreme form of sharing as
something normal and even desirable. However, it is precisely this
*sharing totalitarianism* that 'is killing *cyberflânerie*', because
'the whole point of the *flâneur*'s wanderings' is that one does not
know what he or she cares about.[^3]

On closer inspection, the functioning of Facebook’s timeline is similar
to Google Now (the ‘intelligent and personal’ information service that
supposedly predicts what you want to know), and in fact i t uses the
potentialities of the most popular search engine, along with
localization and access to personal user data, to automatically offer
information and news about the context in which one is situated. The
idea in this case is that such tools, which automatically organize the
information we need, ‘free’ us, as they allow us to focus 'on what's
important' to us. [^4] Such a view implies that,
according to Google, information selection should not be considered a
core activity in the lives of human beings, but a burden that one may
well leave to machines and their algorithms.

By crossing seemingly trivial data – for example, books or records
purchased online – Facebook's tools can help determine, quite precisely,
a profile of the observed user and then establish their political
orientation, gender, religion, etc.[^5] Beyond
legitimate concerns for this additional threat to privacy (already
severely under attack in social networks), perhaps an even greater
danger should be considered: frictionless sharing. In fact, companies
that have interests in common with Facebook could restrict their offer
to those cultural products that are among the favorites of the standard
category to which the user and their contacts/friends have been
assigned. The risk is therefore that individual intellectual horizons
will progressively narrow. If people in social networks encounter only
cultural products that reflect the preferences of the ideal type to
which one belongs according to contemporary marketing, the more
predictable result would be a gradual desertification of the cultural
life of individuals no longer able to encounter what is unusual,
unexpected, and surprising.[^6]

Some of the dynamics behind these narrowing horizons, however, are
typical of social networks based on ‘small worlds’ – for example, those
networks which consist of small groups densely connected to each other.
(The model of the 'small world network' was introduced in 1988 by Duncan
Watts and Steve Strogatz.)[^7] As explained by Clay
Shirky, these networks act as amplifiers and filters of
information.[^8] In other words, because the information
is spread from friends (or contacts) to *friends of friends*, one ends
up stagnating in a flow of information filtered and amplified by one's
contacts. With a closer look, everything that is not checked and
highlighted in one's own small world is destined to go unnoticed.

To sum up, the ideology underlying frictionless sharing is based on the
suppression of all criticism (whether aesthetic, social, or political)
and the consequent reduction of users into 'robots without a soul' who
have the unique function of consuming and therefore producing
statistics, which are in turn fed to software algorithms capable of
transforming them into new stereotypes and abstract consumption models.
[^9]

Similar concerns animate Eli Pariser's reflections on 'filter bubbles',
a concept which represents the universe of information that Google,
Facebook, etc. specifically prearrange for each of us. This U.S.
activist effectively explains how the algorithms underlying the new
conception of the net are increasingly tightening the circle around our
preferences and our desires, to the point that when browsing the
internet, it is unlikely that we will encounter something that has not
been specifically designed for us. This trend provokes great alarm,
because it ends up affecting our ability to choose how we want to spend
our life.[^10]

###Right to Say No

From the perspective of cultural critique, the problematic relationship
of humans with machines that have the capacity to filter cultural
objects has been extensively addressed in the past century and is at the
center of deep reflections on freedom by Vilém Flusser. The Bohemian
thinker (whose horizon of observation is the diffusion of the first
personal computers in the 80s) observes with great concern the
consolidation of a tendency to escape the responsibility of critical
consciousness and instead delegate every decision-making process to
machines. This passage threatens to deprive humans of the critical role
of *essences who make decisions*, [^11] which spells
'the end of freedom'[^12]. It is also not possible to
consider it an expression of freedom when deciding whether to press one
key or another; instead, the decision is based on prescripts and,
ultimately, is a 'programmed freedom' (a choice between prescribed
possibilities). [^13] The escape offered by Flusser is
a utopian telematic society in which it becomes possible to delegate the
production of information to apparatuses in order to be free to devote
oneself to the criticism of the information automatically produced by
them.

Flusser perfectly anticipated the current scenario when he imagined that
in the near future, machines would be calibrated according to informatic
criteria (algorithms) to perform (automatically) the function of
criticism as well. However, it should be underscored that according to
Flusser, the apparatuses – which become ‘critical machines’ – assume the
function of determining which redundant information (kitsch, gossip,
etc.) requires filtering (discarding) and which informative data to let
go , while it is clear that the kinds of experience that prelude the
frictionless sharing philosophy are quite the opposite: redundant
information is not discarded but heavily transmitted over digital
networks. The Facebook timeline, for example, proposes the experience of
continuously redundant content, that is, content that is endlessly
repeated to induce users to conform to them. Moreover, as noted by Bob
Hanke, '[s]ince Flusser's time, we have seen the rise of a myth of user
agency and interactivity, the digital lock-down of culture, and the
enclosure of the information commons.'[^14]

Beyond this aspect, Flusser believes that even in the case of automatic
apparatuses (which have become automatic critics), human beings retain a
'veto right'. The right to say 'no' ( Recht zum Neinsagen ) represents,
in the Flusserian view, the negative decision ( negative Entscheidung )
that we call ‘freedom’. Ultimately, for Flusser, telematics is a
technology of freedom because, while it frees us from having to make
decisions, at the same time it opens the doors to the fundamental
freedom whereby we can say no to telematics itself.
[^15]

###Freedom to Program

Flusser's interrogation into the opportunity to reject telematics and
reassert human intelligence over artificial ones is particularly
challenging in an era such as the present one, in which we are all
struggling to affirm our freedom ('positive freedom') to control the
fate of information (especially information about ourselves), while our
agency is restricted by 'the “black boxing” of technologies, their
networks, and intellectual property regimes'.[^16] For
Flusser, to avoid being programmed by the apparatuses, it is necessary
to devote oneself to their reconfiguration and programming. In other
words it is necessary to say, 'I want to have my program so that I won’t
be subject to anyone else’s'. In a society dominated by 'unidirectional
media' (TV, radio, etc.)*,*senders (broadcasters) possess the programs,
and we are possessed by them; hence, the need to dispossess and
socialize programs emerges. However, in a fully realized**information
society, that is, a society in which centralized senders have been
overcome, it would no longer make sense to speak in terms of
dispossession but rather in terms of dialogical programming. Therefore,
beyond using 'one’s own program', it would be more appropriate to apply
the formula 'programs of others'. In a telematic society, as Flusser
points out, there is no longer the need to possess one’s own program to
reduce the fear of succumbing to someone else’s program. What is more
fundamental is having the 'programs of others' in order to edit (remix)
them and subsequently offer them back.[^17]

In this regard, the popular media theorist Douglas Rushkoff, in his
decalogue *Program or Be Programmed*, writes that '[p]rogramming is the
sweet spot, the high leverage point in a digital society. If we don’t
learn to program, we risk being programmed ourselves.'
[^18] The media theorist notes that if in an earlier
time there were no differences between using a computer and programming
it, today we are confronted with a totally different scenario, and the
goal of those who determine the development of software and hardware
seems to be making interfaces increasingly stratified (below a ‘user
friendly’ interface, the logic of functioning is made inaccessible and
is in fact hidden under different levels of complexity). 'The easy
command-line interface (where you just type a word telling the machine
what you want it to do) was replaced with clicking and dragging and
pointing and watching.' [^19] Put simply, the
computational medium has been made more opaque, thus more similar to
television. Interfaces are becoming ‘user friendly’, but the internal
functioning of machines is hidden in the background.
[^20] The most noteworthy examples are search engines,
which billions of people use to filter information and access news.
These are interfaces with the highest degree of opacity – indeed no one
is able to determine exactly what algorithms they use to function.
[^21] This fact does not diminish their popularity,
thanks to the misperception that the results they offer are ‘objective’
and not determined by user profiling, geographical position, interests
of advertisers, etc., and they are growing at accelerated rates.
[^22]

The most immediate consequence of the tendency to blur interfaces is
that we are learning the features that our computers offer us but not
the operations that we could make them perform. [^23]
In other words, we are satisfied with following the paths drawn for us
by others and do not try to forge our own way.

According to Rushkoff, despite the programming tools at our disposal, we
continue to think in terms of the writing potential of the medium.
Therefore we feel proud and satisfied enough by making a web page or
filling in our profile information page on some social network site. We
continue to be unaware of the biases of the programs we use, as well as
of the ways in which they circumscribe our 'newfound authorship' within
their predetermined agendas. [^24] Our interactivity
is clearly limited when the modalities of interaction have been planned
in advance. Machines are then animated, as Flusser would describe it, by
technologies that represent 'black boxes' known only to those who
program (and whose good faith we are forced to accept on trust).
[^25] As Rushkoff writes:

>Digital technologies are [...] not just objects, but systems embedded
with purpose. They act with intention. If we don’t know how they work,
we won’t even know what they want. The less involved and aware we are of
the way our technologies are programmed and program themselves, the more
narrow our choices will become. [^26]

Rushkoff arrives at a place similar to Flusser's: for both is it clear
that 'the less we will be able to envision alternatives to the pathways
described by our programs […] the more our lives and experiences will be
dictated by their biases'. [^27] Moreover, as
emphasized by Manovich, in a world dominated by software, it is not our
skills alone that have a decisive influence – it is important also to
take into consideration what software tools are used. In fact, what we
can do with a single digital file changes dramatically depending on the
software we have access to. [^28]

Flusser believed that the individual human being can do nothing to
reverse a situation in which we all risk being programmed by
apparatuses. However, 'society as a whole' or as a 'collective brain'
has for Flusser a greater competence than apparatuses that, though
boasting great speed and infallible memories, remain ‘idiotic’. The
media philosopher assumed therefore that it is not individual
'functionaries' [^29] and receivers, but society as
collective brain that has the capacity to regain control of apparatuses
and make judgments about when desirable (informative) situations have
been created.

To avoid being programmed by apparatuses it is necessary that society as
a whole is dedicated to the dialogic reconfiguration and programming of
apparatuses. [^30] But one should never forget, as
Flusser warns, that this is not solely a technical but also a political
question, [^31] and therefore a general agreement
about reconfiguration and programming must become widespread. In
particular, users should be given the opportunity to personally
intervene in the configuration of software platforms they interact with,
so that they can decide for themselves what to share and possibly with
whom; on which information they want software to make suggestions; and
for which cultural objects the filters' operation is considered useful
and which they would prefer to rely on serendipity, that is, random and
unexpected encounters. Ultimately, the rise of a strong political
consensus regarding the inviolability of the right of users is necessary
in order to decide (and to be able to modify at any time the decisions
previously taken) whether and to what extent users want to rely on the
automatic criticism of machines, and whether and to what extent they
want to act as critics, thus as information selectors, instead.

It is also clear that in order for these freedoms to be effective, it is
essential to overcome and leave behind the black box model that makes
companies such as Google and Facebook prosper. The main condition for
the development of a collective dialogue on reconfiguration and
programming is in fact the transparency of the apparatuses with which
one interacts. Therefore any idea of re-appropriating the role of the
critic is condemned to remain an illusion until it is possible again to
look inside our apparatuses, to understand how they work, and to share
such knowledge. [^32]

###References

Becker, Konrad and Felix Stalder (eds). Deep Search: The Politics of
Search beyond Google, Innsbruck: Studienverlag, 2009.

Campanelli, Vito. InfoWar: La battaglia per il controllo e la libertà
della rete , Milano: Egea, 2013.

Castelluccia, Claude. 'Behavioural Tracking on the Internet: A Technical
Perspective', in Serge Gutwirth, Ronald Leenes, Paul De Hert, and Yves
Poullet (eds) European Data Protection: In Good Health? , London and New
York: Springer, 2012: pp. 21-35.

Doctorow, Cory. 'The Coming Civil War over General Purpose Computing',
BoingBoing , August 2012,
http://boingboing.net/2012/08/23/civilwar.html.

Flusser, Vilém. Into the Universe of Technical Images , trans. Nancy Ann
Roth, intro. Mark Poster, Minneapolis and London: University of
Minnesota Press, 2011 (1985).

\_\_\_\_\_. The Shape of Things: A Philosophy of Design, trans. Carl
Hanser, London: Reaktion, 1999 (1993).

Galloway, Alexander R. The Interface Effect , Cambridge: Polity, 2012.

Hanke, Bob. 'Vilém Flusser's Digital Galaxy', International Journal of
Communication (January, 2012): 25-35.

Manovich, Lev. 'There is Only Software', 28 April 2011,
http://www.manovich.net/DOCS/Manovich.there\_is\_only\_software.pdf.

Morozov, Evgeny. 'The Death of the Cyberflâneur', The New York Times , 5
February 2012.

Pariser, Eli. The Filter Bubble: What The Internet Is Hiding From You ,
London and New York: Penguin, 2011.

Rushkoff, Douglas. Program Or be Programmed: Ten Commands for a Digital
Age , New York: ORbooks, 2010.

Shayon, Sheila. 'Facebook Unveils Timeline for ‘Friction-less’
Serendipity', 22 September 2011,
http://www.brandchannel.com/home/post/2011/09/22/Facebook-f8-Timeline-Announcement.aspx.

Shirky, Clay. Here Comes Everybody: How Change Happens when People Come
Together, London and New York: Penguin, 2008.

Vaidhyanathan, Siva. The Googlization of Everything (And Why We Should
Worry) , Los Angeles: University of California Press, 2011.

Watts, Duncan. Small Worlds: The Dynamics of Networks between Order and
Randomness , Princeton: Princeton University Press, 1999.

\_\_\_\_\_. Six Degrees: The Science of a Connected Age , New York:
Norton, 2003.

###Notes

[^1]: Zuckerberg's expression was the following: 'real-time serendipity in
    a friction-less experience'. See Sheila Shayon, 'Facebook Unveils
    Timeline for “Friction-less” Serendipity', 22 September 2011,
    http://www.brandchannel.com/home/post/2011/09/22/Facebook-f8-Timeline-Announcement.aspx.

[^2]: Vito Campanelli, *InfoWar: La battaglia per il controllo e la
    liberta della rete*, Milano: Egea, 2013, p. 115.

[^3]: Evgeny Morozov, 'The Death of the Cyberflâneur', *The New York
    Times*, 5 February 2012, p. 6.

[^4]: On the project's page is written: 'organizing the things you need to
    know and freeing you up to focus on what’s important to you'.
    Retrieved at: http://www.google.it/landing/now.

[^5]: On the state of art of profiling systems, see Claude Castelluccia,
    'Behavioural Tracking on the Internet: A Technical Perspective', in
    Serge Gutwirth, Ronald Leenes, Paul De Hert, Yves Poullet (eds)
    *European Data Protection: In Good Health?*, London and New York:
    Springer, 2012, pp. 21-35.

[^6]: Campanelli, *InfoWar*, pp. 115-116.

[^7]: See Duncan Watts, *Small Worlds: The Dynamics of Networks between
    Order and Randomness*, Princeton: Princeton University Press, 1999.
    See also: Duncan Watts, *Six Degrees: The Science of a Connected
    Age*, New York: Norton, 2003.

[^8]: Clay Shirky, *Here Comes Everybody: How Change Happens when People
    Come Together,* London and New York: Penguin, 2008.

[^9]: Campanelli, *InfoWar*, p. 117.

[^10]: Eli Pariser, *The Filter Bubble: What The Internet Is Hiding From
    You*, London and New York: Penguin, 2011.

[^11]: It's useful to remember that for Flusser 'freedom is essentially
    the difference between that which is redundant and that which is
    actually information, and the free person is the one who is
    competent to decide'. Vilém Flusser, *Ins Universum der technischen
    Bilder*, Göttingen: European Photography, 1985, trans. *Into the
    Universe of Technical Images*, Minneapolis and London: University of
    Minnesota Press, 2011, p. 111.

[^12]: Flusser, *Into the Universe of Technical Images*, p. 119.

[^13]: Vilém Flusser, *The Shape of Things: A Philosophy o* f Design ,
    trans. Carl Hans, London: Reaktion, 1999 (1993), pp. 92-93.

[^14]: Bob Hanke, 'Vilém Flusser's Digital Galaxy', International Journal
    of Communication (January, 2012): 25.

[^15]: Flusser, *Into the Universe*, p. 122.

[^16]: Hanke, 'Vilém Flusser's Digital Galaxy': 25.

[^17]: Flusser, *Into the Universe*, p. 155.

[^18]: Douglas Rushkoff, *Program or Be Programmed: Ten Commands for a
    Digital Age*, New York: ORbooks, 2010, p. 133.

[^19]: Rushkoff, *Program or Be Programmed*, p. 135.

[^20]: On the unworkable nature of all interfaces (windows, doors, screens,
    etc.), see Alexander R. Galloway, *The Interface Effect*, Cambridge:
    Polity, 2012.

[^21]: For a critical analysis of the role of search engines in current
    society and, in particular, of their ability to affect access to
    culture and information, see Konrad Becker and Felix Stalder (eds)
    *Deep Search: The Politics of Search beyond Google*, Innsbruck:
    Studienverlag, 2009, and Siva Vaidhyanathan, *The Googlization of
    Everything (And Why We Should Worry)*, Los Angeles: University of
    California Press, 2011.

[^22]: The statistical analysis company comScore recently documented the
    breaking of the 20 billion searches per month roof, which happened
    in the U.S. market in March 2013. The research is available online
    at
    http://www.comscore.com/Insights/Press\_Releases/2013/4/comScore\_Releases\_March\_2013\_U.S.\_Search\_Engine\_Rankings.

[^23]: Rushkoff, *Program or Be Programmed*, p. 138.

[^24]: Rushkoff, *Program or Be Programmed*, pp. 139-140.

[^25]: Rushkoff, *Program or Be Programmed*, p. 141.

[^26]: Rushkoff, *Program or Be Programmed*, pp. 142-143.

[^27]: Rushkoff, *Program or Be Programmed*, p. 143.

[^28]: Lev Manovich, 'There is Only Software', 28 April 2011,
    http://www.manovich.net/DOCS/Manovich.there\_is\_only\_software.pdf.

[^29]: Flusser writes: 'most apparatuses are not so completely automatic
    that they can get along without human intervention. They need
    functionaries. In this way, the original terms human and apparatus
    are reversed, and human beings operate as a function of the
    apparatus.' Flusser, *Into the Universe*, p. 74.

[^30]: Flusser, *Into the Universe*, pp. 73-77.

[^31]: Flusser, *Into the Universe*, p. 76.

[^32]: These considerations point to the more general issue of general
    purpose computers, a complex subject that has many criticalities
    addressed by the digital rights activist Cory Doctorow, see Cory
    Doctorow, 'The Coming Civil War over General Purpose Computing',
    *BoingBoing*, August 2012,
    http://boingboing.net/2012/08/23/civilwar.html.



